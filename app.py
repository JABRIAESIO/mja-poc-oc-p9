import streamlit as st
import os
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
import io
import time
import glob
import random
import traceback
import json
import requests
import sys
import platform
import psutil

from models.model_loader import load_efficientnet_transformer_model, load_categories, get_model_paths, get_hugging_face_token, HF_MODEL_URL
from models.inference import predict_image
from utils.visualization import plot_prediction_bars
from utils.preprocessing import resize_and_pad_image, apply_data_augmentation

st.set_page_config(
    page_title="Classification d'Images - ConvNeXtTiny",
    page_icon="ü¶ú",
    layout="wide",
    initial_sidebar_state="expanded"
)

def test_hugging_face_connection():
    """
    Teste la connexion √† Hugging Face en v√©rifiant l'acc√®s au mod√®le.
    """
    try:
        # Obtenir le token
        hf_token = get_hugging_face_token()
        
        # Pr√©parer les headers
        headers = {}
        if hf_token:
            headers["Authorization"] = f"Bearer {hf_token}"
            st.info("üîë Token d'authentification trouv√©")
        else:
            st.warning("‚ö†Ô∏è Aucun token d'authentification trouv√©")
        
        # Faire la requ√™te
        with st.spinner("Test en cours..."):
            response = requests.head(HF_MODEL_URL, headers=headers, timeout=10)
            
            if response.status_code == 200:
                size_mb = int(response.headers.get('content-length', 0)) / (1024 * 1024)
                st.success(f"‚úÖ Connexion r√©ussie! \nTaille du mod√®le: {size_mb:.2f} MB")
                return True
            else:
                st.error(f"‚ùå Erreur HTTP {response.status_code}")
                
                # Afficher des informations de d√©bogage
                with st.expander("D√©tails de l'erreur"):
                    st.write({
                        "Status Code": response.status_code,
                        "Headers": dict(response.headers),
                        "URL": HF_MODEL_URL,
                        "Token pr√©sent": bool(hf_token)
                    })
                return False
    except Exception as e:
        st.error(f"‚ùå Erreur: {str(e)}")
        st.exception(e)
        return False

# Informations syst√®me pour le d√©bogage
with st.sidebar:
    st.title("Informations syst√®me")
    st.write(f"Python version: {platform.python_version()}")
    st.write(f"M√©moire disponible: {psutil.virtual_memory().available / (1024 * 1024):.2f} MB")
    st.write(f"CPU count: {os.cpu_count()}")
    st.write(f"Working directory: {os.getcwd()}")

    # Test de connexion √† Hugging Face avec la nouvelle fonction
    if st.button("Tester la connexion √† Hugging Face"):
        test_hugging_face_connection()

st.title("ü¶ú Classification d'Images avec ConvNeXtTiny")
st.markdown("""
Cette application utilise un mod√®le ConvNeXtTiny pour classifier les images.
T√©l√©chargez une image ou utilisez un exemple pour voir le mod√®le en action! Mourad JABRI 2025
""")

@st.cache_resource
def load_model():
    with st.spinner('Chargement du mod√®le... Cela peut prendre quelques instants.'):
        try:
            # Test de connexion √† Hugging Face
            st.info("Test de connexion √† Hugging Face...")
            test_result = test_hugging_face_connection()
            if not test_result:
                st.warning("La connexion √† Hugging Face a √©chou√©, mais nous allons essayer de charger le mod√®le quand m√™me.")

            # Obtention des chemins (fonction maintenue pour compatibilit√©)
            paths = get_model_paths()
            st.write("Chemins recherch√©s:", paths)

            # Mesure du temps de chargement du mod√®le
            start_time = time.time()

            # Utilisation de la fonction load_efficientnet_transformer_model qui redirige vers ConvNeXtTiny
            st.info("Chargement du mod√®le ConvNeXtTiny depuis Hugging Face...")
            model = load_efficientnet_transformer_model()

            end_time = time.time()
            loading_time = end_time - start_time

            if model is None:
                st.error("Le mod√®le n'a pas pu √™tre charg√© (retourn√© None)")
                
                # V√©rifier si le token est pr√©sent
                hf_token = get_hugging_face_token()
                if not hf_token:
                    st.error("Aucun token Hugging Face trouv√©. Veuillez l'ajouter dans les secrets Streamlit.")
                    st.info("Pour ajouter un token Hugging Face, allez dans les param√®tres de votre application Streamlit Cloud, puis dans l'onglet 'Secrets' et ajoutez: HF_TOKEN = 'votre_token'")
                
                return None, None

            # V√©rification du mod√®le charg√©
            st.write("### V√©rification du mod√®le charg√©")
            st.write(f"Nombre de couches: {len(model.layers)}")
            st.write(f"Shape de sortie: {model.output_shape}")
            st.write(f"Temps de chargement: {loading_time:.2f} secondes")

            # Afficher le type de mod√®le charg√©
            import tensorflow as tf
            model_size = sum([tf.keras.backend.count_params(w) for w in model.weights])
            st.write(f"Taille du mod√®le: {model_size/1000000:.2f} M param√®tres")

            if model_size < 10_000_000:  # 10M param√®tres
                st.success("‚úÖ Mod√®le ConvNeXtTiny charg√©")
            else:
                st.info("‚úÖ Mod√®le standard charg√©")

            categories = load_categories()
            st.success("Mod√®le et cat√©gories charg√©s avec succ√®s!")
            return model, categories

        except Exception as e:
            st.error(f"Erreur d√©taill√©e lors du chargement: {str(e)}")
            st.error("Traceback complet:")
            st.code(traceback.format_exc())
            
            # V√©rifier si le token est pr√©sent
            hf_token = get_hugging_face_token()
            if not hf_token:
                st.error("Aucun token Hugging Face trouv√©. Veuillez l'ajouter dans les secrets Streamlit.")
                st.info("Pour ajouter un token Hugging Face, allez dans les param√®tres de votre application Streamlit Cloud, puis dans l'onglet 'Secrets' et ajoutez: HF_TOKEN = 'votre_token'")
            
            return None, None

# Ajoutez ici le reste de votre application, y compris le chargement du mod√®le, 
# l'interface utilisateur pour t√©l√©charger les images, etc.
# Par exemple:

# Chargement du mod√®le
model, categories = load_model()

# Interface utilisateur pour l'upload d'images
uploaded_file = st.file_uploader("Choisissez une image...", type=["jpg", "jpeg", "png"])

if uploaded_file is not None:
    # Traitement de l'image t√©l√©charg√©e
    try:
        image = Image.open(uploaded_file)
        st.image(image, caption="Image t√©l√©charg√©e", use_column_width=True)
        
        # Pr√©diction
        if model is not None and categories is not None:
            with st.spinner("Classification en cours..."):
                predictions = predict_image(model, image, categories)
                
                # Afficher les r√©sultats
                st.subheader("R√©sultats de la classification")
                # Visualisation des r√©sultats
                fig = plot_prediction_bars(predictions)
                st.pyplot(fig)
    except Exception as e:
        st.error(f"Erreur lors du traitement de l'image: {str(e)}")
        st.code(traceback.format_exc())
