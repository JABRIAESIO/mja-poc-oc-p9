import streamlit as st
import os
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
import io
import time
import glob
import random
import traceback
import json
import requests
import sys
import platform
import psutil

from models.model_loader import load_efficientnet_transformer_model, load_categories, get_model_paths
from models.inference import predict_image
from utils.visualization import plot_prediction_bars
from utils.preprocessing import resize_and_pad_image, apply_data_augmentation

st.set_page_config(
    page_title="Classification d'Images - ConvNeXtTiny",
    page_icon="ü¶ú",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Informations syst√®me pour le d√©bogage
with st.sidebar:
    st.title("Informations syst√®me")
    st.write(f"Python version: {platform.python_version()}")
    st.write(f"M√©moire disponible: {psutil.virtual_memory().available / (1024 * 1024):.2f} MB")
    st.write(f"CPU count: {os.cpu_count()}")
    st.write(f"Working directory: {os.getcwd()}")
    
    # Test de connexion √† Hugging Face
    if st.button("Tester la connexion √† Hugging Face"):
        try:
            from models.model_loader import HF_MODEL_URL
            with st.spinner("Test en cours..."):
                response = requests.head(HF_MODEL_URL, timeout=10)
                if response.status_code == 200:
                    size_mb = int(response.headers.get('content-length', 0)) / (1024 * 1024)
                    st.success(f"‚úÖ Connexion r√©ussie! \nTaille du mod√®le: {size_mb:.2f} MB")
                else:
                    st.error(f"‚ùå Erreur HTTP {response.status_code}")
        except Exception as e:
            st.error(f"‚ùå Erreur: {str(e)}")

st.title("ü¶ú Classification d'Images avec ConvNeXtTiny")
st.markdown("""
Cette application utilise un mod√®le ConvNeXtTiny pour classifier les images.
T√©l√©chargez une image ou utilisez un exemple pour voir le mod√®le en action! Mourad JABRI 2025
""")

@st.cache_resource
def load_model():
    with st.spinner('Chargement du mod√®le... Cela peut prendre quelques instants.'):
        try:
            # Test de connexion √† Hugging Face
            st.info("Test de connexion √† Hugging Face...")
            try:
                from models.model_loader import HF_MODEL_URL
                response = requests.head(HF_MODEL_URL, timeout=10)
                if response.status_code == 200:
                    size_mb = int(response.headers.get('content-length', 0)) / (1024 * 1024)
                    st.success(f"‚úÖ Connexion √† Hugging Face r√©ussie! Taille du mod√®le: {size_mb:.2f} MB")
                else:
                    st.error(f"‚ùå Erreur de connexion √† Hugging Face: Code {response.status_code}")
            except Exception as e:
                st.error(f"‚ùå Erreur lors du test de connexion √† Hugging Face: {str(e)}")
                
            # Obtention des chemins (fonction maintenue pour compatibilit√©)
            paths = get_model_paths()
            st.write("Chemins recherch√©s:", paths)

            # Mesure du temps de chargement du mod√®le
            start_time = time.time()
            
            # Utilisation de la fonction load_efficientnet_transformer_model qui redirige vers ConvNeXtTiny
            st.info("Chargement du mod√®le ConvNeXtTiny depuis Hugging Face...")
            model = load_efficientnet_transformer_model()
            
            end_time = time.time()
            loading_time = end_time - start_time

            if model is None:
                st.error("Le mod√®le n'a pas pu √™tre charg√© (retourn√© None)")
                return None, None

            # V√©rification du mod√®le charg√©
            st.write("### V√©rification du mod√®le charg√©")
            st.write(f"Nombre de couches: {len(model.layers)}")
            st.write(f"Shape de sortie: {model.output_shape}")
            st.write(f"Temps de chargement: {loading_time:.2f} secondes")

            # Afficher le type de mod√®le charg√©
            import tensorflow as tf
            model_size = sum([tf.keras.backend.count_params(w) for w in model.weights])
            st.write(f"Taille du mod√®le: {model_size/1000000:.2f} M param√®tres")
            
            if model_size < 10_000_000:  # 10M param√®tres
                st.success("‚úÖ Mod√®le ConvNeXtTiny charg√©")
            else:
                st.info("‚úÖ Mod√®le standard charg√©")

            categories = load_categories()
            st.success("Mod√®le et cat√©gories charg√©s avec succ√®s!")
            return model, categories

        except Exception as e:
            st.error(f"Erreur d√©taill√©e lors du chargement: {str(e)}")
            st.error("Traceback complet:")
            st.code(traceback.format_exc())
            return None, None
